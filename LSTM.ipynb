{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6985c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_dir/classification/tuner0.json\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 65ms/step - accuracy: 0.5819 - auc: 0.5856 - loss: 0.6792 - val_accuracy: 0.5029 - val_auc: 0.5009 - val_loss: 0.7010\n",
      "Epoch 2/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.6204 - auc: 0.6444 - loss: 0.6582 - val_accuracy: 0.5000 - val_auc: 0.5075 - val_loss: 0.7089\n",
      "Epoch 3/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.6246 - auc: 0.6715 - loss: 0.6517 - val_accuracy: 0.5115 - val_auc: 0.5122 - val_loss: 0.7160\n",
      "Epoch 4/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.6119 - auc: 0.6614 - loss: 0.6514 - val_accuracy: 0.5374 - val_auc: 0.5215 - val_loss: 0.7221\n",
      "Epoch 5/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.6427 - auc: 0.7059 - loss: 0.6352 - val_accuracy: 0.5086 - val_auc: 0.5290 - val_loss: 0.7431\n",
      "Epoch 6/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.6582 - auc: 0.7440 - loss: 0.6082 - val_accuracy: 0.5115 - val_auc: 0.5459 - val_loss: 0.7569\n",
      "Test Loss: 0.8666, Accuracy: 0.4516, AUC: 0.4580\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.83      0.58       200\n",
      "           1       0.47      0.13      0.20       234\n",
      "\n",
      "    accuracy                           0.45       434\n",
      "   macro avg       0.46      0.48      0.39       434\n",
      "weighted avg       0.46      0.45      0.38       434\n",
      "\n",
      "[[166  34]\n",
      " [204  30]]\n",
      "Saved classification model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import Huber\n",
    "import kerastuner as kt\n",
    "\n",
    "# Suppress TF warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('data_final.csv')\n",
    "\n",
    "# Binary target: up/down movement\n",
    "data['Direction'] = (data['LogReturn'] > 0).astype(int)\n",
    "\n",
    "y = data['Direction']\n",
    "X = data.drop(['LogReturn', 'Direction', 'date'], axis=1, errors='ignore')\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Sequence creation\n",
    "def make_sequences(X, y, window=30):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(window, len(X)):\n",
    "        Xs.append(X[i-window:i])\n",
    "        ys.append(y.iloc[i])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "WINDOW = 30\n",
    "X_seq, y_seq = make_sequences(X_scaled, y, WINDOW)\n",
    "\n",
    "# Train/test split\n",
    "split = int(len(X_seq) * 0.8)\n",
    "X_train, X_test = X_seq[:split], X_seq[split:]\n",
    "y_train, y_test = y_seq[:split], y_seq[split:]\n",
    "\n",
    "# Attention layer\n",
    "def AttentionLayer():\n",
    "    class Attention(layers.Layer):\n",
    "        def __init__(self, **kwargs): super().__init__(**kwargs)\n",
    "        def build(self, input_shape):\n",
    "            self.W = self.add_weight(shape=(input_shape[-1],), initializer='random_normal', trainable=True)\n",
    "        def call(self, inputs):\n",
    "            att = tf.nn.softmax(tf.tensordot(inputs, self.W, axes=[2,0]), axis=1)\n",
    "            return tf.reduce_sum(inputs * tf.expand_dims(att, -1), axis=1)\n",
    "    return Attention()\n",
    "\n",
    "# Model builder\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Bidirectional(layers.LSTM(hp.Int('lstm1', 64, 256, step=64), return_sequences=True), input_shape=X_train.shape[1:]))\n",
    "    model.add(layers.Bidirectional(layers.GRU(hp.Int('gru1', 64, 256, step=64), return_sequences=True)))\n",
    "    model.add(AttentionLayer())\n",
    "    model.add(layers.Dense(hp.Int('dense', 128, 512, step=128), activation='relu'))\n",
    "    model.add(layers.Dropout(hp.Float('dropout', 0.1, 0.5, step=0.1)))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(hp.Float('lr', 1e-5, 1e-2, sampling='log')),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Hyperparameter Tuning (TimeSeriesSplit CV)\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_auc',\n",
    "    max_trials=15,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_dir',\n",
    "    project_name='classification'\n",
    ")\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_idx, val_idx in tscv.split(X_train):\n",
    "    tuner.search(\n",
    "        X_train[train_idx], y_train[train_idx],\n",
    "        validation_data=(X_train[val_idx], y_train[val_idx]),\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        callbacks=[keras.callbacks.EarlyStopping('val_loss', patience=10), keras.callbacks.ReduceLROnPlateau('val_loss', factor=0.5, patience=5)],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "# Retrieve and train best model\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "best_model = build_model(best_hp)\n",
    "\n",
    "# Fix: Don't use the get() method with default values\n",
    "# Instead, use fixed values for epochs and batch_size\n",
    "best_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,  # Fixed value instead of best_hp.get('tuner/epochs', 50)\n",
    "    batch_size=64,  # Fixed value instead of best_hp.get('tuner/batch_size', 64)\n",
    "    callbacks=[keras.callbacks.EarlyStopping('val_loss', patience=5)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "loss, acc, auc = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss:.4f}, Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "# Detailed metrics\n",
    "y_pred = (best_model.predict(X_test) > 0.5).astype(int)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "best_model.save('best_classification_model.keras')\n",
    "print(\"Saved classification model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40f0e59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_dir/classification/tuner0.json\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 64ms/step - accuracy: 0.5397 - auc: 0.5571 - loss: 0.6870 - val_accuracy: 0.4770 - val_auc: 0.5177 - val_loss: 0.7002\n",
      "Epoch 2/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.6126 - auc: 0.6255 - loss: 0.6627 - val_accuracy: 0.4799 - val_auc: 0.5226 - val_loss: 0.7062\n",
      "Epoch 3/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.6182 - auc: 0.6564 - loss: 0.6529 - val_accuracy: 0.5000 - val_auc: 0.5343 - val_loss: 0.7129\n",
      "Epoch 4/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.6555 - auc: 0.7032 - loss: 0.6293 - val_accuracy: 0.4885 - val_auc: 0.5468 - val_loss: 0.7434\n",
      "Epoch 5/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.6565 - auc: 0.7184 - loss: 0.6231 - val_accuracy: 0.5086 - val_auc: 0.5585 - val_loss: 0.7556\n",
      "Epoch 6/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.6645 - auc: 0.7210 - loss: 0.6245 - val_accuracy: 0.4914 - val_auc: 0.5588 - val_loss: 0.7952\n",
      "\n",
      "=== Model Architecture ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_20\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_20\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_40                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">45,568</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_41                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">74,496</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_40                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m45,568\u001b[0m │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_41                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m74,496\u001b[0m │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_20 (\u001b[38;5;33mAttention\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_40 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">460,421</span> (1.76 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m460,421\u001b[0m (1.76 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">153,473</span> (599.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m153,473\u001b[0m (599.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">306,948</span> (1.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m306,948\u001b[0m (1.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.9598, Accuracy: 0.4447, AUC: 0.4646\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.86      0.59       200\n",
      "           1       0.43      0.09      0.15       234\n",
      "\n",
      "    accuracy                           0.44       434\n",
      "   macro avg       0.44      0.47      0.37       434\n",
      "weighted avg       0.44      0.44      0.35       434\n",
      "\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "[[172  28]\n",
      " [213  21]]\n",
      "\n",
      "=== Feature Importance Analysis ===\n",
      "Analyzing feature importance at most recent time point...\n",
      "Baseline ROC AUC: 0.4649\n",
      "Computing importance for feature 1/24: nonfarm_payrolls\n",
      "Computing importance for feature 2/24: corporate_profits\n",
      "Computing importance for feature 3/24: consumer_confidence\n",
      "Computing importance for feature 4/24: permits\n",
      "Computing importance for feature 5/24: unemployment_lag1\n",
      "Computing importance for feature 6/24: interest_rate_roll3_std\n",
      "Computing importance for feature 7/24: Gold_LogReturn\n",
      "Computing importance for feature 8/24: dayofweek_cos\n",
      "Computing importance for feature 9/24: dayofmonth_sin\n",
      "Computing importance for feature 10/24: dayofyear_sin\n",
      "Computing importance for feature 11/24: dayofyear_cos\n",
      "Computing importance for feature 12/24: MA10\n",
      "Computing importance for feature 13/24: MA50\n",
      "Computing importance for feature 14/24: volatility_10\n",
      "Computing importance for feature 15/24: MACD\n",
      "Computing importance for feature 16/24: BB_lower\n",
      "Computing importance for feature 17/24: RSI\n",
      "Computing importance for feature 18/24: LogReturn_lag1\n",
      "Computing importance for feature 19/24: LogReturn_lag3\n",
      "Computing importance for feature 20/24: LogReturn_lag4\n",
      "Computing importance for feature 21/24: LogReturn_lag5\n",
      "Computing importance for feature 22/24: LogReturn_lag6\n",
      "Computing importance for feature 23/24: LogReturn_lag7\n",
      "Computing importance for feature 24/24: LogReturn_lag10\n",
      "\n",
      "Feature ranking by importance:\n",
      "1. dayofyear_cos: 0.0006 Â± 0.0005\n",
      "2. LogReturn_lag6: 0.0004 Â± 0.0008\n",
      "3. LogReturn_lag4: 0.0003 Â± 0.0005\n",
      "4. LogReturn_lag1: 0.0003 Â± 0.0001\n",
      "5. volatility_10: 0.0003 Â± 0.0002\n",
      "6. MACD: 0.0002 Â± 0.0000\n",
      "7. consumer_confidence: 0.0002 Â± 0.0001\n",
      "8. LogReturn_lag7: 0.0001 Â± 0.0003\n",
      "9. dayofyear_sin: 0.0001 Â± 0.0003\n",
      "10. dayofweek_cos: 0.0000 Â± 0.0003\n",
      "11. BB_lower: 0.0000 Â± 0.0001\n",
      "12. LogReturn_lag10: 0.0000 Â± 0.0002\n",
      "13. unemployment_lag1: 0.0000 Â± 0.0001\n",
      "14. Gold_LogReturn: -0.0000 Â± 0.0002\n",
      "15. permits: -0.0000 Â± 0.0001\n",
      "16. LogReturn_lag5: -0.0001 Â± 0.0005\n",
      "17. nonfarm_payrolls: -0.0001 Â± 0.0001\n",
      "18. LogReturn_lag3: -0.0001 Â± 0.0002\n",
      "19. corporate_profits: -0.0002 Â± 0.0001\n",
      "20. interest_rate_roll3_std: -0.0003 Â± 0.0003\n",
      "Saved feature importance visualization to 'feature_importance.png'\n",
      "\n",
      "Saved classification model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import Huber\n",
    "import kerastuner as kt\n",
    "\n",
    "# Suppress TF warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('data_final.csv')\n",
    "\n",
    "# Binary target: up/down movement\n",
    "data['Direction'] = (data['LogReturn'] > 0).astype(int)\n",
    "\n",
    "y = data['Direction']\n",
    "X = data.drop(['LogReturn', 'Direction', 'date'], axis=1, errors='ignore')\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Sequence creation\n",
    "def make_sequences(X, y, window=30):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(window, len(X)):\n",
    "        Xs.append(X[i-window:i])\n",
    "        ys.append(y.iloc[i])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "WINDOW = 30\n",
    "X_seq, y_seq = make_sequences(X_scaled, y, WINDOW)\n",
    "\n",
    "# Train/test split\n",
    "split = int(len(X_seq) * 0.8)\n",
    "X_train, X_test = X_seq[:split], X_seq[split:]\n",
    "y_train, y_test = y_seq[:split], y_seq[split:]\n",
    "\n",
    "# Attention layer\n",
    "def AttentionLayer():\n",
    "    class Attention(layers.Layer):\n",
    "        def __init__(self, **kwargs): super().__init__(**kwargs)\n",
    "        def build(self, input_shape):\n",
    "            self.W = self.add_weight(shape=(input_shape[-1],), initializer='random_normal', trainable=True)\n",
    "        def call(self, inputs):\n",
    "            att = tf.nn.softmax(tf.tensordot(inputs, self.W, axes=[2,0]), axis=1)\n",
    "            return tf.reduce_sum(inputs * tf.expand_dims(att, -1), axis=1)\n",
    "    return Attention()\n",
    "\n",
    "# Model builder\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Bidirectional(layers.LSTM(hp.Int('lstm1', 64, 256, step=64), return_sequences=True), input_shape=X_train.shape[1:]))\n",
    "    model.add(layers.Bidirectional(layers.GRU(hp.Int('gru1', 64, 256, step=64), return_sequences=True)))\n",
    "    model.add(AttentionLayer())\n",
    "    model.add(layers.Dense(hp.Int('dense', 128, 512, step=128), activation='relu'))\n",
    "    model.add(layers.Dropout(hp.Float('dropout', 0.1, 0.5, step=0.1)))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(hp.Float('lr', 1e-5, 1e-2, sampling='log')),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Hyperparameter Tuning (TimeSeriesSplit CV)\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_auc',\n",
    "    max_trials=15,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_dir',\n",
    "    project_name='classification'\n",
    ")\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_idx, val_idx in tscv.split(X_train):\n",
    "    tuner.search(\n",
    "        X_train[train_idx], y_train[train_idx],\n",
    "        validation_data=(X_train[val_idx], y_train[val_idx]),\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        callbacks=[keras.callbacks.EarlyStopping('val_loss', patience=10), keras.callbacks.ReduceLROnPlateau('val_loss', factor=0.5, patience=5)],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "# Retrieve and train best model\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "best_model = build_model(best_hp)\n",
    "\n",
    "# Fix: Don't use the get() method with default values\n",
    "# Instead, use fixed values for epochs and batch_size\n",
    "best_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,  # Fixed value instead of best_hp.get('tuner/epochs', 50)\n",
    "    batch_size=64,  # Fixed value instead of best_hp.get('tuner/batch_size', 64)\n",
    "    callbacks=[keras.callbacks.EarlyStopping('val_loss', patience=5)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "print(\"\\n=== Model Architecture ===\")\n",
    "best_model.summary()\n",
    "\n",
    "# Evaluate\n",
    "loss, acc, auc = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Loss: {loss:.4f}, Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "# Detailed metrics\n",
    "y_pred = (best_model.predict(X_test) > 0.5).astype(int)\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Feature Importance using custom approach\n",
    "print(\"\\n=== Feature Importance Analysis ===\")\n",
    "\n",
    "# Get feature names\n",
    "feature_names = X.columns if hasattr(X, 'columns') else [f\"Feature_{i}\" for i in range(X.shape[1])]\n",
    "\n",
    "# Analyze feature importance at the most recent time point (most relevant for prediction)\n",
    "print(\"Analyzing feature importance at most recent time point...\")\n",
    "X_recent = X_test[:, -1, :]  # Most recent data point in each sequence\n",
    "\n",
    "# Manual implementation of permutation importance\n",
    "n_features = X_recent.shape[1]\n",
    "n_repeats = 5\n",
    "importances = np.zeros(n_features)\n",
    "importances_std = np.zeros(n_features)\n",
    "\n",
    "# Get baseline score with original data\n",
    "baseline_preds = best_model.predict(X_test, verbose=0)\n",
    "baseline_score = roc_auc_score(y_test, baseline_preds)\n",
    "print(f\"Baseline ROC AUC: {baseline_score:.4f}\")\n",
    "\n",
    "# Compute feature importance\n",
    "for feature_idx in range(n_features):\n",
    "    print(f\"Computing importance for feature {feature_idx+1}/{n_features}: {feature_names[feature_idx] if feature_idx < len(feature_names) else 'Feature_'+str(feature_idx)}\")\n",
    "    \n",
    "    feature_scores = []\n",
    "    \n",
    "    # Repeat permutation multiple times\n",
    "    for _ in range(n_repeats):\n",
    "        # Create a copy of the test data\n",
    "        X_test_permuted = X_test.copy()\n",
    "        \n",
    "        # Permute the values of the feature at the most recent time step\n",
    "        permuted_values = X_recent[:, feature_idx].copy()\n",
    "        np.random.shuffle(permuted_values)\n",
    "        X_test_permuted[:, -1, feature_idx] = permuted_values\n",
    "        \n",
    "        # Get predictions with permuted feature\n",
    "        permuted_preds = best_model.predict(X_test_permuted, verbose=0)\n",
    "        \n",
    "        # Calculate score with permuted feature\n",
    "        permuted_score = roc_auc_score(y_test, permuted_preds)\n",
    "        \n",
    "        # Calculate importance as decrease in performance\n",
    "        importance = baseline_score - permuted_score\n",
    "        feature_scores.append(importance)\n",
    "    \n",
    "    # Store mean and std of importance scores\n",
    "    importances[feature_idx] = np.mean(feature_scores)\n",
    "    importances_std[feature_idx] = np.std(feature_scores)\n",
    "\n",
    "\n",
    "\n",
    "# Sort features by importance\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print feature ranking\n",
    "print(\"\\nFeature ranking by importance:\")\n",
    "for i, idx in enumerate(indices):\n",
    "    if i < 20:  # Print top 20 features\n",
    "        feature_name = feature_names[idx] if idx < len(feature_names) else f\"Feature_{idx}\"\n",
    "        print(f\"{i+1}. {feature_name}: {importances[idx]:.4f} Â± {importances_std[idx]:.4f}\")\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Feature Importance (Top 15)\")\n",
    "top_indices = indices[:15]\n",
    "plt.barh(range(len(top_indices)), importances[top_indices], color=\"r\", yerr=importances_std[top_indices], align=\"center\")\n",
    "plt.yticks(range(len(top_indices)), [feature_names[i] if i < len(feature_names) else f\"Feature_{i}\" for i in top_indices])\n",
    "plt.ylim([-1, len(top_indices)])\n",
    "plt.xlabel(\"Feature Importance (Mean Decrease in AUC)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png')\n",
    "plt.close()\n",
    "print(\"Saved feature importance visualization to 'feature_importance.png'\")\n",
    "\n",
    "# Save model\n",
    "best_model.save('best_classification_model.keras')\n",
    "print(\"\\nSaved classification model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3bc4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 15:57:28.282593: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-13 15:57:28.444868: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-13 15:57:28.521672: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747151848.624170    1790 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747151848.651665    1790 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747151848.842233    1790 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747151848.842269    1790 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747151848.842272    1790 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747151848.842275    1790 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-13 15:57:28.861837: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/tmp/ipykernel_1790/4081929880.py:15: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  import kerastuner as kt\n",
      "2025-05-13 15:57:31.668606: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/home/codespace/.python/current/lib/python3.12/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n",
      "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"/home/codespace/.python/current/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n",
      "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.python/current/lib/python3.12/site-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n",
      "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.python/current/lib/python3.12/site-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n",
      "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.python/current/lib/python3.12/site-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n",
      "    return model.fit(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.python/current/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/home/codespace/.python/current/lib/python3.12/site-packages/keras_tuner/src/engine/tuner_utils.py\", line 76, in on_epoch_end\n",
      "    self._save_model()\n",
      "  File \"/home/codespace/.python/current/lib/python3.12/site-packages/keras_tuner/src/engine/tuner_utils.py\", line 86, in _save_model\n",
      "    self.model.save_weights(write_filepath)\n",
      "  File \"/home/codespace/.python/current/lib/python3.12/site-packages/h5py/_hl/files.py\", line 564, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.python/current/lib/python3.12/site-packages/h5py/_hl/files.py\", line 244, in make_fid\n",
      "    fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 122, in h5py.h5f.create\n",
      "FileNotFoundError: [Errno 2] Unable to synchronously create file (unable to open file: name = 'tuner_dir/classification/trial_04/checkpoint.weights.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 138ms/step - accuracy: 0.5270 - auc: 0.5318 - loss: 0.6900 - val_accuracy: 0.5287 - val_auc: 0.4855 - val_loss: 0.7052\n",
      "Epoch 2/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.5890 - auc: 0.6259 - loss: 0.6647 - val_accuracy: 0.4713 - val_auc: 0.4926 - val_loss: 0.7147\n",
      "Epoch 3/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.6193 - auc: 0.6640 - loss: 0.6517 - val_accuracy: 0.5057 - val_auc: 0.5016 - val_loss: 0.7217\n",
      "Epoch 4/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.6240 - auc: 0.6737 - loss: 0.6448 - val_accuracy: 0.4943 - val_auc: 0.4975 - val_loss: 0.7432\n",
      "Epoch 5/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.6092 - auc: 0.6833 - loss: 0.6397 - val_accuracy: 0.5172 - val_auc: 0.5178 - val_loss: 0.7580\n",
      "Epoch 6/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.6519 - auc: 0.6867 - loss: 0.6339 - val_accuracy: 0.5000 - val_auc: 0.5120 - val_loss: 0.8122\n",
      "\n",
      "=== Model Architecture ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">45,568</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">592,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">196,992</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">385</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m45,568\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │       \u001b[38;5;34m592,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_1 (\u001b[38;5;33mAttention\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)            │       \u001b[38;5;34m196,992\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m385\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,509,061</span> (9.57 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,509,061\u001b[0m (9.57 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">836,353</span> (3.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m836,353\u001b[0m (3.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,672,708</span> (6.38 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m1,672,708\u001b[0m (6.38 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.9574, Accuracy: 0.4447, AUC: 0.5240\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.84      0.58       200\n",
      "           1       0.44      0.10      0.17       234\n",
      "\n",
      "    accuracy                           0.44       434\n",
      "   macro avg       0.44      0.47      0.37       434\n",
      "weighted avg       0.44      0.44      0.36       434\n",
      "\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "[[169  31]\n",
      " [210  24]]\n",
      "\n",
      "Accuracy: 0.4447\n",
      "Precision: 0.4364\n",
      "Recall: 0.1026\n",
      "\n",
      "=== Feature Importance Analysis ===\n",
      "Analyzing feature importance at most recent time point...\n",
      "Baseline ROC AUC: 0.5232\n",
      "Computing importance for feature 1/24: nonfarm_payrolls\n",
      "Computing importance for feature 2/24: corporate_profits\n",
      "Computing importance for feature 3/24: consumer_confidence\n",
      "Computing importance for feature 4/24: permits\n",
      "Computing importance for feature 5/24: unemployment_lag1\n",
      "Computing importance for feature 6/24: interest_rate_roll3_std\n",
      "Computing importance for feature 7/24: Gold_LogReturn\n",
      "Computing importance for feature 8/24: dayofweek_cos\n",
      "Computing importance for feature 9/24: dayofmonth_sin\n",
      "Computing importance for feature 10/24: dayofyear_sin\n",
      "Computing importance for feature 11/24: dayofyear_cos\n",
      "Computing importance for feature 12/24: MA10\n",
      "Computing importance for feature 13/24: MA50\n",
      "Computing importance for feature 14/24: volatility_10\n",
      "Computing importance for feature 15/24: MACD\n",
      "Computing importance for feature 16/24: BB_lower\n",
      "Computing importance for feature 17/24: RSI\n",
      "Computing importance for feature 18/24: LogReturn_lag1\n",
      "Computing importance for feature 19/24: LogReturn_lag3\n",
      "Computing importance for feature 20/24: LogReturn_lag4\n",
      "Computing importance for feature 21/24: LogReturn_lag5\n",
      "Computing importance for feature 22/24: LogReturn_lag6\n",
      "Computing importance for feature 23/24: LogReturn_lag7\n",
      "Computing importance for feature 24/24: LogReturn_lag10\n",
      "\n",
      "Feature ranking by importance:\n",
      "1. LogReturn_lag1: 0.0017 Â± 0.0004\n",
      "2. dayofyear_cos: 0.0017 Â± 0.0006\n",
      "3. interest_rate_roll3_std: 0.0009 Â± 0.0004\n",
      "4. LogReturn_lag7: 0.0005 Â± 0.0009\n",
      "5. LogReturn_lag4: 0.0003 Â± 0.0002\n",
      "6. dayofyear_sin: 0.0002 Â± 0.0001\n",
      "7. LogReturn_lag5: 0.0002 Â± 0.0004\n",
      "8. LogReturn_lag10: 0.0002 Â± 0.0001\n",
      "9. consumer_confidence: 0.0001 Â± 0.0001\n",
      "10. unemployment_lag1: 0.0000 Â± 0.0000\n",
      "11. LogReturn_lag6: 0.0000 Â± 0.0002\n",
      "12. nonfarm_payrolls: -0.0001 Â± 0.0000\n",
      "13. Gold_LogReturn: -0.0001 Â± 0.0005\n",
      "14. permits: -0.0001 Â± 0.0002\n",
      "15. dayofweek_cos: -0.0002 Â± 0.0003\n",
      "16. volatility_10: -0.0002 Â± 0.0001\n",
      "17. corporate_profits: -0.0003 Â± 0.0002\n",
      "18. MA10: -0.0004 Â± 0.0005\n",
      "19. LogReturn_lag3: -0.0005 Â± 0.0004\n",
      "20. MACD: -0.0006 Â± 0.0003\n",
      "Saved feature importance visualization to 'feature_importance.png'\n",
      "\n",
      "Saved classification model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import Huber\n",
    "import kerastuner as kt\n",
    "\n",
    "# Suppress TF warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('data_final.csv')\n",
    "\n",
    "# Binary target: up/down movement\n",
    "data['Direction'] = (data['LogReturn'] > 0).astype(int)\n",
    "\n",
    "y = data['Direction']\n",
    "X = data.drop(['LogReturn', 'Direction', 'date'], axis=1, errors='ignore')\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Sequence creation\n",
    "def make_sequences(X, y, window=30):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(window, len(X)):\n",
    "        Xs.append(X[i-window:i])\n",
    "        ys.append(y.iloc[i])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "WINDOW = 30\n",
    "X_seq, y_seq = make_sequences(X_scaled, y, WINDOW)\n",
    "\n",
    "# Train/test split\n",
    "split = int(len(X_seq) * 0.8)\n",
    "X_train, X_test = X_seq[:split], X_seq[split:]\n",
    "y_train, y_test = y_seq[:split], y_seq[split:]\n",
    "\n",
    "# Attention layer\n",
    "def AttentionLayer():\n",
    "    class Attention(layers.Layer):\n",
    "        def __init__(self, **kwargs): super().__init__(**kwargs)\n",
    "        def build(self, input_shape):\n",
    "            self.W = self.add_weight(shape=(input_shape[-1],), initializer='random_normal', trainable=True)\n",
    "        def call(self, inputs):\n",
    "            att = tf.nn.softmax(tf.tensordot(inputs, self.W, axes=[2,0]), axis=1)\n",
    "            return tf.reduce_sum(inputs * tf.expand_dims(att, -1), axis=1)\n",
    "    return Attention()\n",
    "\n",
    "# Model builder\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Bidirectional(layers.LSTM(hp.Int('lstm1', 64, 256, step=64), return_sequences=True), input_shape=X_train.shape[1:]))\n",
    "    model.add(layers.Bidirectional(layers.GRU(hp.Int('gru1', 64, 256, step=64), return_sequences=True)))\n",
    "    model.add(AttentionLayer())\n",
    "    model.add(layers.Dense(hp.Int('dense', 128, 512, step=128), activation='relu'))\n",
    "    model.add(layers.Dropout(hp.Float('dropout', 0.1, 0.5, step=0.1)))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(hp.Float('lr', 1e-5, 1e-2, sampling='log')),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Hyperparameter Tuning (TimeSeriesSplit CV)\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_auc',\n",
    "    max_trials=15,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_dir',\n",
    "    project_name='classification'\n",
    ")\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_idx, val_idx in tscv.split(X_train):\n",
    "    tuner.search(\n",
    "        X_train[train_idx], y_train[train_idx],\n",
    "        validation_data=(X_train[val_idx], y_train[val_idx]),\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        callbacks=[keras.callbacks.EarlyStopping('val_loss', patience=10), keras.callbacks.ReduceLROnPlateau('val_loss', factor=0.5, patience=5)],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "# Retrieve and train best model\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "best_model = build_model(best_hp)\n",
    "\n",
    "# Fix: Don't use the get() method with default values\n",
    "# Instead, use fixed values for epochs and batch_size\n",
    "best_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,  # Fixed value instead of best_hp.get('tuner/epochs', 50)\n",
    "    batch_size=64,  # Fixed value instead of best_hp.get('tuner/batch_size', 64)\n",
    "    callbacks=[keras.callbacks.EarlyStopping('val_loss', patience=5)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "print(\"\\n=== Model Architecture ===\")\n",
    "best_model.summary()\n",
    "\n",
    "# Evaluate\n",
    "loss, acc, auc = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Loss: {loss:.4f}, Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "# Detailed metrics\n",
    "y_pred = (best_model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy, precision, recall\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "# Feature Importance using custom approach\n",
    "print(\"\\n=== Feature Importance Analysis ===\")\n",
    "\n",
    "# Get feature names\n",
    "feature_names = X.columns if hasattr(X, 'columns') else [f\"Feature_{i}\" for i in range(X.shape[1])]\n",
    "\n",
    "# Analyze feature importance at the most recent time point (most relevant for prediction)\n",
    "print(\"Analyzing feature importance at most recent time point...\")\n",
    "X_recent = X_test[:, -1, :]  # Most recent data point in each sequence\n",
    "\n",
    "# Manual implementation of permutation importance\n",
    "n_features = X_recent.shape[1]\n",
    "n_repeats = 5\n",
    "importances = np.zeros(n_features)\n",
    "importances_std = np.zeros(n_features)\n",
    "\n",
    "# Get baseline score with original data\n",
    "baseline_preds = best_model.predict(X_test, verbose=0)\n",
    "baseline_score = roc_auc_score(y_test, baseline_preds)\n",
    "print(f\"Baseline ROC AUC: {baseline_score:.4f}\")\n",
    "\n",
    "# Compute feature importance\n",
    "for feature_idx in range(n_features):\n",
    "    print(f\"Computing importance for feature {feature_idx+1}/{n_features}: {feature_names[feature_idx] if feature_idx < len(feature_names) else 'Feature_'+str(feature_idx)}\")\n",
    "    \n",
    "    feature_scores = []\n",
    "    \n",
    "    # Repeat permutation multiple times\n",
    "    for _ in range(n_repeats):\n",
    "        # Create a copy of the test data\n",
    "        X_test_permuted = X_test.copy()\n",
    "        \n",
    "        # Permute the values of the feature at the most recent time step\n",
    "        permuted_values = X_recent[:, feature_idx].copy()\n",
    "        np.random.shuffle(permuted_values)\n",
    "        X_test_permuted[:, -1, feature_idx] = permuted_values\n",
    "        \n",
    "        # Get predictions with permuted feature\n",
    "        permuted_preds = best_model.predict(X_test_permuted, verbose=0)\n",
    "        \n",
    "        # Calculate score with permuted feature\n",
    "        permuted_score = roc_auc_score(y_test, permuted_preds)\n",
    "        \n",
    "        # Calculate importance as decrease in performance\n",
    "        importance = baseline_score - permuted_score\n",
    "        feature_scores.append(importance)\n",
    "    \n",
    "    # Store mean and std of importance scores\n",
    "    importances[feature_idx] = np.mean(feature_scores)\n",
    "    importances_std[feature_idx] = np.std(feature_scores)\n",
    "\n",
    "# Sort features by importance\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print feature ranking\n",
    "print(\"\\nFeature ranking by importance:\")\n",
    "for i, idx in enumerate(indices):\n",
    "    if i < 20:  # Print top 20 features\n",
    "        feature_name = feature_names[idx] if idx < len(feature_names) else f\"Feature_{idx}\"\n",
    "        print(f\"{i+1}. {feature_name}: {importances[idx]:.4f} Â± {importances_std[idx]:.4f}\")\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Feature Importance (Top 15)\")\n",
    "top_indices = indices[:15]\n",
    "plt.barh(range(len(top_indices)), importances[top_indices], color=\"r\", yerr=importances_std[top_indices], align=\"center\")\n",
    "plt.yticks(range(len(top_indices)), [feature_names[i] if i < len(feature_names) else f\"Feature_{i}\" for i in top_indices])\n",
    "plt.ylim([-1, len(top_indices)])\n",
    "plt.xlabel(\"Feature Importance (Mean Decrease in AUC)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png')\n",
    "plt.close()\n",
    "print(\"Saved feature importance visualization to 'feature_importance.png'\")\n",
    "\n",
    "# Save model\n",
    "best_model.save('best_classification_model.keras')\n",
    "print(\"\\nSaved classification model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "155c0191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_dir/regression_close/tuner0.json\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 179ms/step - loss: 0.0478 - mae: 0.2083 - val_loss: 0.2391 - val_mae: 0.6039\n",
      "Epoch 2/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 166ms/step - loss: 0.0038 - mae: 0.0678 - val_loss: 0.2204 - val_mae: 0.5845\n",
      "Epoch 3/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 167ms/step - loss: 0.0026 - mae: 0.0558 - val_loss: 0.2024 - val_mae: 0.5632\n",
      "Epoch 4/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 164ms/step - loss: 0.0022 - mae: 0.0516 - val_loss: 0.1892 - val_mae: 0.5465\n",
      "Epoch 5/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 164ms/step - loss: 0.0017 - mae: 0.0453 - val_loss: 0.1815 - val_mae: 0.5322\n",
      "Epoch 6/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 165ms/step - loss: 0.0017 - mae: 0.0451 - val_loss: 0.1729 - val_mae: 0.5216\n",
      "Epoch 7/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 165ms/step - loss: 0.0014 - mae: 0.0415 - val_loss: 0.1707 - val_mae: 0.5176\n",
      "Epoch 8/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 165ms/step - loss: 0.0013 - mae: 0.0404 - val_loss: 0.1581 - val_mae: 0.4954\n",
      "Epoch 9/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 165ms/step - loss: 0.0013 - mae: 0.0386 - val_loss: 0.1464 - val_mae: 0.4784\n",
      "Epoch 10/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 169ms/step - loss: 0.0014 - mae: 0.0414 - val_loss: 0.1469 - val_mae: 0.4803\n",
      "Epoch 11/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 165ms/step - loss: 0.0011 - mae: 0.0367 - val_loss: 0.1392 - val_mae: 0.4654\n",
      "Epoch 12/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 165ms/step - loss: 0.0011 - mae: 0.0365 - val_loss: 0.1377 - val_mae: 0.4610\n",
      "Epoch 13/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 164ms/step - loss: 0.0011 - mae: 0.0361 - val_loss: 0.1351 - val_mae: 0.4605\n",
      "Epoch 14/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 173ms/step - loss: 0.0011 - mae: 0.0353 - val_loss: 0.1294 - val_mae: 0.4506\n",
      "Epoch 15/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 202ms/step - loss: 9.6891e-04 - mae: 0.0343 - val_loss: 0.1253 - val_mae: 0.4406\n",
      "Epoch 16/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 165ms/step - loss: 8.8137e-04 - mae: 0.0328 - val_loss: 0.1243 - val_mae: 0.4396\n",
      "Epoch 17/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 165ms/step - loss: 8.8743e-04 - mae: 0.0328 - val_loss: 0.1221 - val_mae: 0.4356\n",
      "Epoch 18/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 165ms/step - loss: 8.9115e-04 - mae: 0.0326 - val_loss: 0.1208 - val_mae: 0.4334\n",
      "Epoch 19/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 163ms/step - loss: 9.1326e-04 - mae: 0.0330 - val_loss: 0.1160 - val_mae: 0.4255\n",
      "Epoch 20/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 170ms/step - loss: 9.0222e-04 - mae: 0.0325 - val_loss: 0.1056 - val_mae: 0.4053\n",
      "Epoch 21/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 165ms/step - loss: 7.8398e-04 - mae: 0.0309 - val_loss: 0.1090 - val_mae: 0.4114\n",
      "Epoch 22/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 164ms/step - loss: 8.4946e-04 - mae: 0.0317 - val_loss: 0.1046 - val_mae: 0.4009\n",
      "Epoch 23/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 164ms/step - loss: 9.0441e-04 - mae: 0.0325 - val_loss: 0.0977 - val_mae: 0.3887\n",
      "Epoch 24/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 164ms/step - loss: 7.5506e-04 - mae: 0.0298 - val_loss: 0.0954 - val_mae: 0.3831\n",
      "Epoch 25/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 164ms/step - loss: 8.5043e-04 - mae: 0.0316 - val_loss: 0.0962 - val_mae: 0.3852\n",
      "Epoch 26/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 162ms/step - loss: 7.8044e-04 - mae: 0.0304 - val_loss: 0.0945 - val_mae: 0.3827\n",
      "Epoch 27/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 166ms/step - loss: 7.8409e-04 - mae: 0.0306 - val_loss: 0.0911 - val_mae: 0.3767\n",
      "Epoch 28/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 164ms/step - loss: 8.7573e-04 - mae: 0.0323 - val_loss: 0.0884 - val_mae: 0.3678\n",
      "Epoch 29/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 166ms/step - loss: 7.1407e-04 - mae: 0.0288 - val_loss: 0.0945 - val_mae: 0.3820\n",
      "Epoch 30/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 165ms/step - loss: 8.0450e-04 - mae: 0.0305 - val_loss: 0.0863 - val_mae: 0.3642\n",
      "Epoch 31/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 165ms/step - loss: 8.0803e-04 - mae: 0.0308 - val_loss: 0.0928 - val_mae: 0.3777\n",
      "Epoch 32/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 163ms/step - loss: 7.1846e-04 - mae: 0.0288 - val_loss: 0.0832 - val_mae: 0.3597\n",
      "Epoch 33/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 165ms/step - loss: 7.5571e-04 - mae: 0.0293 - val_loss: 0.0835 - val_mae: 0.3581\n",
      "Epoch 34/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 164ms/step - loss: 7.9372e-04 - mae: 0.0304 - val_loss: 0.0798 - val_mae: 0.3484\n",
      "Epoch 35/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 165ms/step - loss: 7.0739e-04 - mae: 0.0284 - val_loss: 0.0865 - val_mae: 0.3662\n",
      "Epoch 36/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 164ms/step - loss: 6.6517e-04 - mae: 0.0278 - val_loss: 0.0862 - val_mae: 0.3648\n",
      "Epoch 37/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 167ms/step - loss: 7.8808e-04 - mae: 0.0295 - val_loss: 0.0817 - val_mae: 0.3541\n",
      "Epoch 38/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 166ms/step - loss: 6.9525e-04 - mae: 0.0283 - val_loss: 0.0818 - val_mae: 0.3532\n",
      "Epoch 39/50\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 166ms/step - loss: 6.8846e-04 - mae: 0.0284 - val_loss: 0.0826 - val_mae: 0.3559\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "\n",
      "Test MSE: 0.317896\n",
      "Test RMSE: 0.563823\n",
      "Test MAE: 0.424755\n",
      "Test R²: -0.1262\n",
      "Saved Close price prediction model.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import Huber\n",
    "import kerastuner as kt\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('transformed_data.csv')\n",
    "\n",
    "# Use Close as the target variable for regression\n",
    "y = data['LogReturn'].values\n",
    "X = data.drop(['LogReturn', 'Direction', 'date', 'Close'], axis=1, errors='ignore')\n",
    "\n",
    "# Scale features (X)\n",
    "X_scaler = StandardScaler()\n",
    "X_scaled = X_scaler.fit_transform(X)\n",
    "\n",
    "# Scale target (y = Close)\n",
    "y_scaler = StandardScaler()\n",
    "y_scaled = y_scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Sequence creation\n",
    "def make_sequences(X, y, window=30):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(window, len(X)):\n",
    "        Xs.append(X[i-window:i])\n",
    "        ys.append(y[i])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "WINDOW = 30\n",
    "X_seq, y_seq = make_sequences(X_scaled, y_scaled, WINDOW)\n",
    "\n",
    "# Train/test split\n",
    "split = int(len(X_seq) * 0.8)\n",
    "X_train, X_test = X_seq[:split], X_seq[split:]\n",
    "y_train, y_test = y_seq[:split], y_seq[split:]\n",
    "\n",
    "# Save original (unscaled) y_test values for final evaluation\n",
    "_, y_orig = make_sequences(X_scaled, y, WINDOW)\n",
    "y_test_orig = y_orig[split:]\n",
    "\n",
    "# Attention Layer\n",
    "def AttentionLayer():\n",
    "    class Attention(layers.Layer):\n",
    "        def __init__(self, **kwargs): super().__init__(**kwargs)\n",
    "        def build(self, input_shape):\n",
    "            self.W = self.add_weight(shape=(input_shape[-1],), initializer='random_normal', trainable=True)\n",
    "        def call(self, inputs):\n",
    "            att = tf.nn.softmax(tf.tensordot(inputs, self.W, axes=[2,0]), axis=1)\n",
    "            return tf.reduce_sum(inputs * tf.expand_dims(att, -1), axis=1)\n",
    "    return Attention()\n",
    "\n",
    "# Model builder\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Bidirectional(layers.LSTM(hp.Int('lstm1', 64, 256, step=64), return_sequences=True), input_shape=X_train.shape[1:]))\n",
    "    model.add(layers.Bidirectional(layers.GRU(hp.Int('gru1', 64, 256, step=64), return_sequences=True)))\n",
    "    model.add(AttentionLayer())\n",
    "    model.add(layers.Dense(hp.Int('dense', 128, 512, step=128), activation='relu'))\n",
    "    model.add(layers.Dropout(hp.Float('dropout', 0.1, 0.5, step=0.1)))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(hp.Float('lr', 1e-5, 1e-2, sampling='log')),\n",
    "        loss=Huber(),\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Hyperparameter tuning\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_mae',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_dir',\n",
    "    project_name='regression_close'\n",
    ")\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "for train_idx, val_idx in tscv.split(X_train):\n",
    "    tuner.search(\n",
    "        X_train[train_idx], y_train[train_idx],\n",
    "        validation_data=(X_train[val_idx], y_train[val_idx]),\n",
    "        epochs=50,\n",
    "        batch_size=64,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping('val_loss', patience=5),\n",
    "            keras.callbacks.ReduceLROnPlateau('val_loss', factor=0.5, patience=3)\n",
    "        ],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "# Train final model\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "best_model = build_model(best_hp)\n",
    "\n",
    "best_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[keras.callbacks.EarlyStopping('val_loss', patience=5)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict and inverse transform\n",
    "y_pred_scaled = best_model.predict(X_test).flatten()\n",
    "y_pred = y_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test_orig, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_orig, y_pred)\n",
    "r2 = r2_score(y_test_orig, y_pred)\n",
    "\n",
    "print(f\"\\nTest MSE: {mse:.6f}\")\n",
    "print(f\"Test RMSE: {rmse:.6f}\")\n",
    "print(f\"Test MAE: {mae:.6f}\")\n",
    "print(f\"Test R²: {r2:.4f}\")\n",
    "\n",
    "# Save model\n",
    "best_model.save('best_close_model.keras')\n",
    "print(\"Saved Close price prediction model.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c400372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: -1.7209867234379428\n",
      "Max: 2.1450165824050687\n"
     ]
    }
   ],
   "source": [
    "print(\"Min:\", y_scaled.min())\n",
    "print(\"Max:\", y_scaled.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d383dbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_dir/regression_logreturn/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 195ms/step - loss: 0.3737 - mae: 0.6900 - val_loss: 0.2250 - val_mae: 0.4754\n",
      "Epoch 2/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 169ms/step - loss: 0.0157 - mae: 0.1403 - val_loss: 0.3874 - val_mae: 0.6491\n",
      "Epoch 3/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 172ms/step - loss: 0.0121 - mae: 0.1234 - val_loss: 0.3169 - val_mae: 0.5418\n",
      "Epoch 4/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 170ms/step - loss: 0.0107 - mae: 0.1168 - val_loss: 0.2789 - val_mae: 0.4971\n",
      "Epoch 5/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 169ms/step - loss: 0.0082 - mae: 0.1001 - val_loss: 0.2389 - val_mae: 0.4500\n",
      "Epoch 6/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 172ms/step - loss: 0.0079 - mae: 0.0985 - val_loss: 0.2469 - val_mae: 0.4762\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
      "\n",
      "Test MSE: 10.615928\n",
      "Test RMSE: 3.258209\n",
      "Test MAE: 2.892967\n",
      "Test R²: -630.4324\n",
      "Saved LogReturn prediction model.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import Huber\n",
    "import kerastuner as kt\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('transformed_data.csv')\n",
    "\n",
    "# Use LogReturn as the target variable\n",
    "y = data['LogReturn'].values\n",
    "X = data.drop(['LogReturn', 'Direction', 'date', 'Close'], axis=1, errors='ignore')\n",
    "\n",
    "# Scale features (X)\n",
    "X_scaler = StandardScaler()\n",
    "X_scaled = X_scaler.fit_transform(X)\n",
    "\n",
    "# Do NOT scale target (y)\n",
    "y_scaled = y  # Keep y unscaled\n",
    "\n",
    "# Sequence creation\n",
    "def make_sequences(X, y, window=30):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(window, len(X)):\n",
    "        Xs.append(X[i-window:i])\n",
    "        ys.append(y[i])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "WINDOW = 30\n",
    "X_seq, y_seq = make_sequences(X_scaled, y_scaled, WINDOW)\n",
    "\n",
    "# Train/test split\n",
    "split = int(len(X_seq) * 0.8)\n",
    "X_train, X_test = X_seq[:split], X_seq[split:]\n",
    "y_train, y_test = y_seq[:split], y_seq[split:]\n",
    "\n",
    "# Save original y_test for final evaluation\n",
    "_, y_orig = make_sequences(X_scaled, y, WINDOW)\n",
    "y_test_orig = y_orig[split:]\n",
    "\n",
    "# Attention Layer\n",
    "def AttentionLayer():\n",
    "    class Attention(layers.Layer):\n",
    "        def __init__(self, **kwargs): super().__init__(**kwargs)\n",
    "        def build(self, input_shape):\n",
    "            self.W = self.add_weight(shape=(input_shape[-1],), initializer='random_normal', trainable=True)\n",
    "        def call(self, inputs):\n",
    "            att = tf.nn.softmax(tf.tensordot(inputs, self.W, axes=[2,0]), axis=1)\n",
    "            return tf.reduce_sum(inputs * tf.expand_dims(att, -1), axis=1)\n",
    "    return Attention()\n",
    "\n",
    "# Model builder\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Bidirectional(layers.LSTM(hp.Int('lstm1', 64, 256, step=64), return_sequences=True), input_shape=X_train.shape[1:]))\n",
    "    model.add(layers.Bidirectional(layers.GRU(hp.Int('gru1', 64, 256, step=64), return_sequences=True)))\n",
    "    model.add(AttentionLayer())\n",
    "    model.add(layers.Dense(hp.Int('dense', 128, 512, step=128), activation='relu'))\n",
    "    model.add(layers.Dropout(hp.Float('dropout', 0.1, 0.5, step=0.1)))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(hp.Float('lr', 1e-5, 1e-2, sampling='log')),\n",
    "        loss=Huber(),\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Hyperparameter tuning\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_mae',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_dir',\n",
    "    project_name='regression_logreturn'\n",
    ")\n",
    "\n",
    "# TimeSeries Cross Validation\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "for train_idx, val_idx in tscv.split(X_train):\n",
    "    tuner.search(\n",
    "        X_train[train_idx], y_train[train_idx],\n",
    "        validation_data=(X_train[val_idx], y_train[val_idx]),\n",
    "        epochs=50,\n",
    "        batch_size=64,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping('val_loss', patience=5),\n",
    "            keras.callbacks.ReduceLROnPlateau('val_loss', factor=0.5, patience=3)\n",
    "        ],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "# Train final model\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "best_model = build_model(best_hp)\n",
    "\n",
    "best_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[keras.callbacks.EarlyStopping('val_loss', patience=5)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict (no inverse transform needed)\n",
    "y_pred = best_model.predict(X_test).flatten()\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test_orig, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_orig, y_pred)\n",
    "r2 = r2_score(y_test_orig, y_pred)\n",
    "\n",
    "print(f\"\\nTest MSE: {mse:.6f}\")\n",
    "print(f\"Test RMSE: {rmse:.6f}\")\n",
    "print(f\"Test MAE: {mae:.6f}\")\n",
    "print(f\"Test R²: {r2:.4f}\")\n",
    "\n",
    "# Save model\n",
    "best_model.save('best_logreturn_model.keras')\n",
    "print(\"Saved LogReturn prediction model.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
