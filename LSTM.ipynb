{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6985c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_dir/classification/tuner0.json\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 65ms/step - accuracy: 0.5819 - auc: 0.5856 - loss: 0.6792 - val_accuracy: 0.5029 - val_auc: 0.5009 - val_loss: 0.7010\n",
      "Epoch 2/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.6204 - auc: 0.6444 - loss: 0.6582 - val_accuracy: 0.5000 - val_auc: 0.5075 - val_loss: 0.7089\n",
      "Epoch 3/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.6246 - auc: 0.6715 - loss: 0.6517 - val_accuracy: 0.5115 - val_auc: 0.5122 - val_loss: 0.7160\n",
      "Epoch 4/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.6119 - auc: 0.6614 - loss: 0.6514 - val_accuracy: 0.5374 - val_auc: 0.5215 - val_loss: 0.7221\n",
      "Epoch 5/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.6427 - auc: 0.7059 - loss: 0.6352 - val_accuracy: 0.5086 - val_auc: 0.5290 - val_loss: 0.7431\n",
      "Epoch 6/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.6582 - auc: 0.7440 - loss: 0.6082 - val_accuracy: 0.5115 - val_auc: 0.5459 - val_loss: 0.7569\n",
      "Test Loss: 0.8666, Accuracy: 0.4516, AUC: 0.4580\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.83      0.58       200\n",
      "           1       0.47      0.13      0.20       234\n",
      "\n",
      "    accuracy                           0.45       434\n",
      "   macro avg       0.46      0.48      0.39       434\n",
      "weighted avg       0.46      0.45      0.38       434\n",
      "\n",
      "[[166  34]\n",
      " [204  30]]\n",
      "Saved classification model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import Huber\n",
    "import kerastuner as kt\n",
    "\n",
    "# Suppress TF warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('data_final.csv')\n",
    "\n",
    "# Binary target: up/down movement\n",
    "data['Direction'] = (data['LogReturn'] > 0).astype(int)\n",
    "\n",
    "y = data['Direction']\n",
    "X = data.drop(['LogReturn', 'Direction', 'date'], axis=1, errors='ignore')\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Sequence creation\n",
    "def make_sequences(X, y, window=30):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(window, len(X)):\n",
    "        Xs.append(X[i-window:i])\n",
    "        ys.append(y.iloc[i])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "WINDOW = 30\n",
    "X_seq, y_seq = make_sequences(X_scaled, y, WINDOW)\n",
    "\n",
    "# Train/test split\n",
    "split = int(len(X_seq) * 0.8)\n",
    "X_train, X_test = X_seq[:split], X_seq[split:]\n",
    "y_train, y_test = y_seq[:split], y_seq[split:]\n",
    "\n",
    "# Attention layer\n",
    "def AttentionLayer():\n",
    "    class Attention(layers.Layer):\n",
    "        def __init__(self, **kwargs): super().__init__(**kwargs)\n",
    "        def build(self, input_shape):\n",
    "            self.W = self.add_weight(shape=(input_shape[-1],), initializer='random_normal', trainable=True)\n",
    "        def call(self, inputs):\n",
    "            att = tf.nn.softmax(tf.tensordot(inputs, self.W, axes=[2,0]), axis=1)\n",
    "            return tf.reduce_sum(inputs * tf.expand_dims(att, -1), axis=1)\n",
    "    return Attention()\n",
    "\n",
    "# Model builder\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Bidirectional(layers.LSTM(hp.Int('lstm1', 64, 256, step=64), return_sequences=True), input_shape=X_train.shape[1:]))\n",
    "    model.add(layers.Bidirectional(layers.GRU(hp.Int('gru1', 64, 256, step=64), return_sequences=True)))\n",
    "    model.add(AttentionLayer())\n",
    "    model.add(layers.Dense(hp.Int('dense', 128, 512, step=128), activation='relu'))\n",
    "    model.add(layers.Dropout(hp.Float('dropout', 0.1, 0.5, step=0.1)))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(hp.Float('lr', 1e-5, 1e-2, sampling='log')),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Hyperparameter Tuning (TimeSeriesSplit CV)\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_auc',\n",
    "    max_trials=15,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_dir',\n",
    "    project_name='classification'\n",
    ")\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_idx, val_idx in tscv.split(X_train):\n",
    "    tuner.search(\n",
    "        X_train[train_idx], y_train[train_idx],\n",
    "        validation_data=(X_train[val_idx], y_train[val_idx]),\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        callbacks=[keras.callbacks.EarlyStopping('val_loss', patience=10), keras.callbacks.ReduceLROnPlateau('val_loss', factor=0.5, patience=5)],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "# Retrieve and train best model\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "best_model = build_model(best_hp)\n",
    "\n",
    "# Fix: Don't use the get() method with default values\n",
    "# Instead, use fixed values for epochs and batch_size\n",
    "best_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,  # Fixed value instead of best_hp.get('tuner/epochs', 50)\n",
    "    batch_size=64,  # Fixed value instead of best_hp.get('tuner/batch_size', 64)\n",
    "    callbacks=[keras.callbacks.EarlyStopping('val_loss', patience=5)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "loss, acc, auc = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss:.4f}, Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "# Detailed metrics\n",
    "y_pred = (best_model.predict(X_test) > 0.5).astype(int)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "best_model.save('best_classification_model.keras')\n",
    "print(\"Saved classification model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40f0e59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_dir/classification/tuner0.json\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 64ms/step - accuracy: 0.5397 - auc: 0.5571 - loss: 0.6870 - val_accuracy: 0.4770 - val_auc: 0.5177 - val_loss: 0.7002\n",
      "Epoch 2/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.6126 - auc: 0.6255 - loss: 0.6627 - val_accuracy: 0.4799 - val_auc: 0.5226 - val_loss: 0.7062\n",
      "Epoch 3/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.6182 - auc: 0.6564 - loss: 0.6529 - val_accuracy: 0.5000 - val_auc: 0.5343 - val_loss: 0.7129\n",
      "Epoch 4/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.6555 - auc: 0.7032 - loss: 0.6293 - val_accuracy: 0.4885 - val_auc: 0.5468 - val_loss: 0.7434\n",
      "Epoch 5/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.6565 - auc: 0.7184 - loss: 0.6231 - val_accuracy: 0.5086 - val_auc: 0.5585 - val_loss: 0.7556\n",
      "Epoch 6/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.6645 - auc: 0.7210 - loss: 0.6245 - val_accuracy: 0.4914 - val_auc: 0.5588 - val_loss: 0.7952\n",
      "\n",
      "=== Model Architecture ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_20\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_20\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_40                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">45,568</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_41                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">74,496</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_40                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m45,568\u001b[0m │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_41                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m74,496\u001b[0m │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_20 (\u001b[38;5;33mAttention\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_40 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">460,421</span> (1.76 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m460,421\u001b[0m (1.76 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">153,473</span> (599.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m153,473\u001b[0m (599.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">306,948</span> (1.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m306,948\u001b[0m (1.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.9598, Accuracy: 0.4447, AUC: 0.4646\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.86      0.59       200\n",
      "           1       0.43      0.09      0.15       234\n",
      "\n",
      "    accuracy                           0.44       434\n",
      "   macro avg       0.44      0.47      0.37       434\n",
      "weighted avg       0.44      0.44      0.35       434\n",
      "\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "[[172  28]\n",
      " [213  21]]\n",
      "\n",
      "=== Feature Importance Analysis ===\n",
      "Analyzing feature importance at most recent time point...\n",
      "Baseline ROC AUC: 0.4649\n",
      "Computing importance for feature 1/24: nonfarm_payrolls\n",
      "Computing importance for feature 2/24: corporate_profits\n",
      "Computing importance for feature 3/24: consumer_confidence\n",
      "Computing importance for feature 4/24: permits\n",
      "Computing importance for feature 5/24: unemployment_lag1\n",
      "Computing importance for feature 6/24: interest_rate_roll3_std\n",
      "Computing importance for feature 7/24: Gold_LogReturn\n",
      "Computing importance for feature 8/24: dayofweek_cos\n",
      "Computing importance for feature 9/24: dayofmonth_sin\n",
      "Computing importance for feature 10/24: dayofyear_sin\n",
      "Computing importance for feature 11/24: dayofyear_cos\n",
      "Computing importance for feature 12/24: MA10\n",
      "Computing importance for feature 13/24: MA50\n",
      "Computing importance for feature 14/24: volatility_10\n",
      "Computing importance for feature 15/24: MACD\n",
      "Computing importance for feature 16/24: BB_lower\n",
      "Computing importance for feature 17/24: RSI\n",
      "Computing importance for feature 18/24: LogReturn_lag1\n",
      "Computing importance for feature 19/24: LogReturn_lag3\n",
      "Computing importance for feature 20/24: LogReturn_lag4\n",
      "Computing importance for feature 21/24: LogReturn_lag5\n",
      "Computing importance for feature 22/24: LogReturn_lag6\n",
      "Computing importance for feature 23/24: LogReturn_lag7\n",
      "Computing importance for feature 24/24: LogReturn_lag10\n",
      "\n",
      "Feature ranking by importance:\n",
      "1. dayofyear_cos: 0.0006 Â± 0.0005\n",
      "2. LogReturn_lag6: 0.0004 Â± 0.0008\n",
      "3. LogReturn_lag4: 0.0003 Â± 0.0005\n",
      "4. LogReturn_lag1: 0.0003 Â± 0.0001\n",
      "5. volatility_10: 0.0003 Â± 0.0002\n",
      "6. MACD: 0.0002 Â± 0.0000\n",
      "7. consumer_confidence: 0.0002 Â± 0.0001\n",
      "8. LogReturn_lag7: 0.0001 Â± 0.0003\n",
      "9. dayofyear_sin: 0.0001 Â± 0.0003\n",
      "10. dayofweek_cos: 0.0000 Â± 0.0003\n",
      "11. BB_lower: 0.0000 Â± 0.0001\n",
      "12. LogReturn_lag10: 0.0000 Â± 0.0002\n",
      "13. unemployment_lag1: 0.0000 Â± 0.0001\n",
      "14. Gold_LogReturn: -0.0000 Â± 0.0002\n",
      "15. permits: -0.0000 Â± 0.0001\n",
      "16. LogReturn_lag5: -0.0001 Â± 0.0005\n",
      "17. nonfarm_payrolls: -0.0001 Â± 0.0001\n",
      "18. LogReturn_lag3: -0.0001 Â± 0.0002\n",
      "19. corporate_profits: -0.0002 Â± 0.0001\n",
      "20. interest_rate_roll3_std: -0.0003 Â± 0.0003\n",
      "Saved feature importance visualization to 'feature_importance.png'\n",
      "\n",
      "Saved classification model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import Huber\n",
    "import kerastuner as kt\n",
    "\n",
    "# Suppress TF warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('data_final.csv')\n",
    "\n",
    "# Binary target: up/down movement\n",
    "data['Direction'] = (data['LogReturn'] > 0).astype(int)\n",
    "\n",
    "y = data['Direction']\n",
    "X = data.drop(['LogReturn', 'Direction', 'date'], axis=1, errors='ignore')\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Sequence creation\n",
    "def make_sequences(X, y, window=30):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(window, len(X)):\n",
    "        Xs.append(X[i-window:i])\n",
    "        ys.append(y.iloc[i])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "WINDOW = 30\n",
    "X_seq, y_seq = make_sequences(X_scaled, y, WINDOW)\n",
    "\n",
    "# Train/test split\n",
    "split = int(len(X_seq) * 0.8)\n",
    "X_train, X_test = X_seq[:split], X_seq[split:]\n",
    "y_train, y_test = y_seq[:split], y_seq[split:]\n",
    "\n",
    "# Attention layer\n",
    "def AttentionLayer():\n",
    "    class Attention(layers.Layer):\n",
    "        def __init__(self, **kwargs): super().__init__(**kwargs)\n",
    "        def build(self, input_shape):\n",
    "            self.W = self.add_weight(shape=(input_shape[-1],), initializer='random_normal', trainable=True)\n",
    "        def call(self, inputs):\n",
    "            att = tf.nn.softmax(tf.tensordot(inputs, self.W, axes=[2,0]), axis=1)\n",
    "            return tf.reduce_sum(inputs * tf.expand_dims(att, -1), axis=1)\n",
    "    return Attention()\n",
    "\n",
    "# Model builder\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Bidirectional(layers.LSTM(hp.Int('lstm1', 64, 256, step=64), return_sequences=True), input_shape=X_train.shape[1:]))\n",
    "    model.add(layers.Bidirectional(layers.GRU(hp.Int('gru1', 64, 256, step=64), return_sequences=True)))\n",
    "    model.add(AttentionLayer())\n",
    "    model.add(layers.Dense(hp.Int('dense', 128, 512, step=128), activation='relu'))\n",
    "    model.add(layers.Dropout(hp.Float('dropout', 0.1, 0.5, step=0.1)))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(hp.Float('lr', 1e-5, 1e-2, sampling='log')),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Hyperparameter Tuning (TimeSeriesSplit CV)\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_auc',\n",
    "    max_trials=15,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_dir',\n",
    "    project_name='classification'\n",
    ")\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_idx, val_idx in tscv.split(X_train):\n",
    "    tuner.search(\n",
    "        X_train[train_idx], y_train[train_idx],\n",
    "        validation_data=(X_train[val_idx], y_train[val_idx]),\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        callbacks=[keras.callbacks.EarlyStopping('val_loss', patience=10), keras.callbacks.ReduceLROnPlateau('val_loss', factor=0.5, patience=5)],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "# Retrieve and train best model\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "best_model = build_model(best_hp)\n",
    "\n",
    "# Fix: Don't use the get() method with default values\n",
    "# Instead, use fixed values for epochs and batch_size\n",
    "best_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,  # Fixed value instead of best_hp.get('tuner/epochs', 50)\n",
    "    batch_size=64,  # Fixed value instead of best_hp.get('tuner/batch_size', 64)\n",
    "    callbacks=[keras.callbacks.EarlyStopping('val_loss', patience=5)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "print(\"\\n=== Model Architecture ===\")\n",
    "best_model.summary()\n",
    "\n",
    "# Evaluate\n",
    "loss, acc, auc = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Loss: {loss:.4f}, Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "# Detailed metrics\n",
    "y_pred = (best_model.predict(X_test) > 0.5).astype(int)\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Feature Importance using custom approach\n",
    "print(\"\\n=== Feature Importance Analysis ===\")\n",
    "\n",
    "# Get feature names\n",
    "feature_names = X.columns if hasattr(X, 'columns') else [f\"Feature_{i}\" for i in range(X.shape[1])]\n",
    "\n",
    "# Analyze feature importance at the most recent time point (most relevant for prediction)\n",
    "print(\"Analyzing feature importance at most recent time point...\")\n",
    "X_recent = X_test[:, -1, :]  # Most recent data point in each sequence\n",
    "\n",
    "# Manual implementation of permutation importance\n",
    "n_features = X_recent.shape[1]\n",
    "n_repeats = 5\n",
    "importances = np.zeros(n_features)\n",
    "importances_std = np.zeros(n_features)\n",
    "\n",
    "# Get baseline score with original data\n",
    "baseline_preds = best_model.predict(X_test, verbose=0)\n",
    "baseline_score = roc_auc_score(y_test, baseline_preds)\n",
    "print(f\"Baseline ROC AUC: {baseline_score:.4f}\")\n",
    "\n",
    "# Compute feature importance\n",
    "for feature_idx in range(n_features):\n",
    "    print(f\"Computing importance for feature {feature_idx+1}/{n_features}: {feature_names[feature_idx] if feature_idx < len(feature_names) else 'Feature_'+str(feature_idx)}\")\n",
    "    \n",
    "    feature_scores = []\n",
    "    \n",
    "    # Repeat permutation multiple times\n",
    "    for _ in range(n_repeats):\n",
    "        # Create a copy of the test data\n",
    "        X_test_permuted = X_test.copy()\n",
    "        \n",
    "        # Permute the values of the feature at the most recent time step\n",
    "        permuted_values = X_recent[:, feature_idx].copy()\n",
    "        np.random.shuffle(permuted_values)\n",
    "        X_test_permuted[:, -1, feature_idx] = permuted_values\n",
    "        \n",
    "        # Get predictions with permuted feature\n",
    "        permuted_preds = best_model.predict(X_test_permuted, verbose=0)\n",
    "        \n",
    "        # Calculate score with permuted feature\n",
    "        permuted_score = roc_auc_score(y_test, permuted_preds)\n",
    "        \n",
    "        # Calculate importance as decrease in performance\n",
    "        importance = baseline_score - permuted_score\n",
    "        feature_scores.append(importance)\n",
    "    \n",
    "    # Store mean and std of importance scores\n",
    "    importances[feature_idx] = np.mean(feature_scores)\n",
    "    importances_std[feature_idx] = np.std(feature_scores)\n",
    "\n",
    "\n",
    "\n",
    "# Sort features by importance\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print feature ranking\n",
    "print(\"\\nFeature ranking by importance:\")\n",
    "for i, idx in enumerate(indices):\n",
    "    if i < 20:  # Print top 20 features\n",
    "        feature_name = feature_names[idx] if idx < len(feature_names) else f\"Feature_{idx}\"\n",
    "        print(f\"{i+1}. {feature_name}: {importances[idx]:.4f} Â± {importances_std[idx]:.4f}\")\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Feature Importance (Top 15)\")\n",
    "top_indices = indices[:15]\n",
    "plt.barh(range(len(top_indices)), importances[top_indices], color=\"r\", yerr=importances_std[top_indices], align=\"center\")\n",
    "plt.yticks(range(len(top_indices)), [feature_names[i] if i < len(feature_names) else f\"Feature_{i}\" for i in top_indices])\n",
    "plt.ylim([-1, len(top_indices)])\n",
    "plt.xlabel(\"Feature Importance (Mean Decrease in AUC)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png')\n",
    "plt.close()\n",
    "print(\"Saved feature importance visualization to 'feature_importance.png'\")\n",
    "\n",
    "# Save model\n",
    "best_model.save('best_classification_model.keras')\n",
    "print(\"\\nSaved classification model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
