{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a811380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 16s]\n",
      "val_root_mean_squared_error: 1.3376317024230957\n",
      "\n",
      "Best val_root_mean_squared_error So Far: 1.1905879974365234\n",
      "Total elapsed time: 00h 03m 40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 36 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 1.059721, MAE: 0.831852, R2: -0.0139\n",
      "Saved improved model\n"
     ]
    }
   ],
   "source": [
    "# Improved Time-Series Classification Pipeline\n",
    "# - Incorporates bidirectional LSTM, attention mechanism, and hyperparameter tuning\n",
    "# - Adds learning rate scheduling and model checkpointing\n",
    "# - Uses keras Tuner for search\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Load preprocessed data\n",
    "data = pd.read_csv('transformed_data.csv', parse_dates=['date'] if 'date' in pd.read_csv('transformed_data.csv', nrows=1).columns else None)\n",
    "\n",
    "# Split features and target\n",
    "y = data['LogReturn']\n",
    "X = data.drop(['LogReturn', 'date'], axis=1, errors='ignore')\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Create sequences\n",
    "def make_sequences(X, y, window):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(window, len(X)):\n",
    "        X_seq.append(X[i-window:i])\n",
    "        y_seq.append(y.iloc[i])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "WINDOW = 30\n",
    "X_seq, y_seq = make_sequences(X_scaled, y, WINDOW)\n",
    "\n",
    "# Time-based split\n",
    "split = int(len(X_seq) * 0.8)\n",
    "X_train, X_test = X_seq[:split], X_seq[split:]\n",
    "y_train, y_test = y_seq[:split], y_seq[split:]\n",
    "\n",
    "# Attention layer definition\n",
    "class Attention(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],), initializer='random_normal', trainable=True)\n",
    "    def call(self, inputs):\n",
    "        weights = tf.nn.softmax(tf.tensordot(inputs, self.W, axes=[2,0]), axis=1)\n",
    "        context = tf.reduce_sum(inputs * tf.expand_dims(weights, -1), axis=1)\n",
    "        return context\n",
    "\n",
    "# Model builder for tuning\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Bidirectional(layers.LSTM(hp.Int('units1', 32, 128, step=32), return_sequences=True), input_shape=X_train.shape[1:]))\n",
    "    model.add(layers.Bidirectional(layers.LSTM(hp.Int('units2', 32, 128, step=32), return_sequences=True)))\n",
    "    model.add(Attention())\n",
    "    model.add(layers.Dense(hp.Int('dense', 64, 256, step=64), activation='relu'))\n",
    "    model.add(layers.Dropout(hp.Float('dropout', 0.2, 0.5, step=0.1)))\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(hp.Float('lr', 1e-4, 1e-2, sampling='log')),\n",
    "        loss='mse',\n",
    "        metrics=[keras.metrics.RootMeanSquaredError(), 'mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Hyperparameter search\n",
    "import kerastuner as kt\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_root_mean_squared_error',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_dir',\n",
    "    project_name='ts_forecast'\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),\n",
    "               keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)]\n",
    ")\n",
    "\n",
    "# Retrieve best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Evaluate\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test RMSE: {rmse:.6f}, MAE: {mae:.6f}, R2: {r2:.4f}\")\n",
    "\n",
    "# Save\n",
    "best_model.save('best_bidirectional_attention_model.h5')\n",
    "print(\"Saved improved model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "502699f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "HyperParameters.get() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 108\u001b[39m\n\u001b[32m    103\u001b[39m best_hp = tuner.get_best_hyperparameters()[\u001b[32m0\u001b[39m]\n\u001b[32m    104\u001b[39m best_model = build_model(best_hp)\n\u001b[32m    105\u001b[39m best_model.fit(\n\u001b[32m    106\u001b[39m     X_train, y_train,\n\u001b[32m    107\u001b[39m     validation_split=\u001b[32m0.2\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     epochs=\u001b[43mbest_hp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtuner/epochs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[32m    109\u001b[39m     batch_size=best_hp.get(\u001b[33m'\u001b[39m\u001b[33mtuner/batch_size\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m64\u001b[39m),\n\u001b[32m    110\u001b[39m     callbacks=[keras.callbacks.EarlyStopping(\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m, patience=\u001b[32m5\u001b[39m)],\n\u001b[32m    111\u001b[39m     verbose=\u001b[32m1\u001b[39m\n\u001b[32m    112\u001b[39m )\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m    115\u001b[39m loss, acc, auc = best_model.evaluate(X_test, y_test, verbose=\u001b[32m0\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: HyperParameters.get() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# Improved Time-Series Classification Pipeline (Binary)\n",
    "# - Classify price movement: up/down\n",
    "# - Bidirectional LSTM + GRU + Attention\n",
    "# - Hyperparameter tuning with Keras Tuner\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import Huber\n",
    "import kerastuner as kt\n",
    "\n",
    "# Suppress TF warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('transformed_data.csv', parse_dates=['date'] if 'date' in pd.read_csv('transformed_data.csv', nrows=1).columns else None)\n",
    "\n",
    "# Binary target: up/down movement\n",
    "data['Direction'] = (data['LogReturn'] > 0).astype(int)\n",
    "\n",
    "y = data['Direction']\n",
    "X = data.drop(['LogReturn', 'Direction', 'date'], axis=1, errors='ignore')\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Sequence creation\n",
    "def make_sequences(X, y, window=30):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(window, len(X)):\n",
    "        Xs.append(X[i-window:i])\n",
    "        ys.append(y.iloc[i])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "WINDOW = 30\n",
    "X_seq, y_seq = make_sequences(X_scaled, y, WINDOW)\n",
    "\n",
    "# Train/test split\n",
    "split = int(len(X_seq) * 0.8)\n",
    "X_train, X_test = X_seq[:split], X_seq[split:]\n",
    "y_train, y_test = y_seq[:split], y_seq[split:]\n",
    "\n",
    "# Attention layer\n",
    "def AttentionLayer():\n",
    "    class Attention(layers.Layer):\n",
    "        def __init__(self, **kwargs): super().__init__(**kwargs)\n",
    "        def build(self, input_shape):\n",
    "            self.W = self.add_weight(shape=(input_shape[-1],), initializer='random_normal', trainable=True)\n",
    "        def call(self, inputs):\n",
    "            att = tf.nn.softmax(tf.tensordot(inputs, self.W, axes=[2,0]), axis=1)\n",
    "            return tf.reduce_sum(inputs * tf.expand_dims(att, -1), axis=1)\n",
    "    return Attention()\n",
    "\n",
    "# Model builder\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Bidirectional(layers.LSTM(hp.Int('lstm1', 64, 256, step=64), return_sequences=True), input_shape=X_train.shape[1:]))\n",
    "    model.add(layers.Bidirectional(layers.GRU(hp.Int('gru1', 64, 256, step=64), return_sequences=True)))\n",
    "    model.add(AttentionLayer())\n",
    "    model.add(layers.Dense(hp.Int('dense', 128, 512, step=128), activation='relu'))\n",
    "    model.add(layers.Dropout(hp.Float('dropout', 0.1, 0.5, step=0.1)))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(hp.Float('lr', 1e-5, 1e-2, sampling='log')),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Hyperparameter Tuning (TimeSeriesSplit CV)\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_auc',\n",
    "    max_trials=15,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_dir',\n",
    "    project_name='classification'\n",
    ")\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_idx, val_idx in tscv.split(X_train):\n",
    "    tuner.search(\n",
    "        X_train[train_idx], y_train[train_idx],\n",
    "        validation_data=(X_train[val_idx], y_train[val_idx]),\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        callbacks=[keras.callbacks.EarlyStopping('val_loss', patience=10), keras.callbacks.ReduceLROnPlateau('val_loss', factor=0.5, patience=5)],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "# Retrieve and train best model\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "best_model = build_model(best_hp)\n",
    "best_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=best_hp.get('tuner/epochs', 50),\n",
    "    batch_size=best_hp.get('tuner/batch_size', 64),\n",
    "    callbacks=[keras.callbacks.EarlyStopping('val_loss', patience=5)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "loss, acc, auc = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss:.4f}, Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "# Detailed metrics\n",
    "y_pred = (best_model.predict(X_test) > 0.5).astype(int)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "best_model.save('best_classification_model.keras')\n",
    "print(\"Saved classification model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
